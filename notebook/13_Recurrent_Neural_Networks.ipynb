{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNj8E/Yv+Xhzva6yGtXRBXy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimgeonhee317/nlpdemystifed-notes/blob/main/notebook/13_Recurrent_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13_Recurrent Neural Networks"
      ],
      "metadata": {
        "id": "z9u4MJ2Pjliq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Library"
      ],
      "metadata": {
        "id": "idcV9Ofejsq0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8_LFKcmAZlM9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from nltk.corpus import treebank, brown, conll2000\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-of-Speech Tagger with Bidirectional LSTM"
      ],
      "metadata": {
        "id": "moqKQkM4kTWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PoS tagging with LSTM is multiclass classification task for sequence.\n",
        "\n",
        "# nltk offers free sets for labelled corpora.\n",
        "# look at https://www.nltk.org/nltk_data\n",
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "nltk.download('conll2000')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVb3M8AIkUFD",
        "outputId": "e333328e-14aa-411e-aa83-85785e69991e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download tagset\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf1e3Hykk10t",
        "outputId": "575978bf-50e3-40c8-addd-803d03d1ce67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download all PoS-tagged sentences and place them in one List.\n",
        "tagged_sentences = treebank.tagged_sents(tagset='universal')+\\\n",
        "                   brown.tagged_sents(tagset='universal')+\\\n",
        "                   conll2000.tagged_sents(tagset='universal')\n",
        "print(tagged_sentences[0])\n",
        "print(f\"Dataset size: {len(tagged_sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_SILV6OlagZ",
        "outputId": "552b3a18-a383-470d-af69-47d3995a055c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n",
            "Dataset size: 72202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, sentence_tags = [], []\n",
        "\n",
        "for s in tagged_sentences:\n",
        "  sentence, tags = zip(*s) # multiple numbers of tuple according to sentences\n",
        "  sentences.append(list(sentence))\n",
        "  sentence_tags.append(list(tags))"
      ],
      "metadata": {
        "id": "fsG84AxsmBrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[0])\n",
        "print(sentence_tags[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuDNua06oEzi",
        "outputId": "b90ea104-9888-4a15-ac3e-834afdc8ca73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
            "['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentences), len(sentence_tags)) # number of sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIgrWPASoJ8p",
        "outputId": "1a29ba2d-f0fd-4b49-c13d-35e6a45ee05c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72202 72202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset according to the proportion below\n",
        "train_ratio = 0.75\n",
        "validation_ratio = 0.15\n",
        "test_ratio=0.10\n",
        "\n",
        "# train:test = 0.75:0.25\n",
        "x_train, x_test, y_train, y_test = train_test_split(sentences, sentence_tags,\n",
        "                                                     test_size= 1-train_ratio,\n",
        "                                                     random_state = 317)\n",
        "# train:val:test = 0.75:0.15:0.10\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test,\n",
        "                                                test_size = test_ratio/(test_ratio + validation_ratio),\n",
        "                                                random_state = 317)"
      ],
      "metadata": {
        "id": "m64dEOEto4RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train), len(y_train))\n",
        "print(len(x_val), len(y_val))\n",
        "print(len(x_test), len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oahgHkABp6-k",
        "outputId": "b5b2a14f-6275-43ba-cd48-a6adc1e0bd4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54151 54151\n",
            "10830 10830\n",
            "7221 7221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate wordvectors for our sentenses\n",
        "# default tokenizer, out-ov-vocabulary token as <OOV>\n",
        "sentence_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<OOV')\n",
        "sentence_tokenizer.fit_on_texts(x_train)\n",
        "print(f\"Vocabulary size: {len(sentence_tokenizer.word_index)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7XFgJzTqB2u",
        "outputId": "bcecb5d1-2072-4ee8-b796-62f9a5d1481b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 52183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need another tokenizer for the tags are also sequences.\n",
        "tag_tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(y_train)"
      ],
      "metadata": {
        "id": "BtzyVYOurvYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of PoS tags: {len(tag_tokenizer.word_index)}\\n\")\n",
        "tag_tokenizer.get_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arfXHis_sioK",
        "outputId": "487e2607-c91c-4916-c6ee-1022027f61cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of PoS tags: 12\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_words': None,\n",
              " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
              " 'lower': True,\n",
              " 'split': ' ',\n",
              " 'char_level': False,\n",
              " 'oov_token': None,\n",
              " 'document_count': 54151,\n",
              " 'word_counts': '{\"adv\": 51392, \"verb\": 175631, \"adp\": 137365, \"det\": 127600, \".\": 143593, \"adj\": 81110, \"noun\": 288313, \"conj\": 35420, \"num\": 21374, \"prt\": 31340, \"pron\": 44737, \"x\": 6109}',\n",
              " 'word_docs': '{\"conj\": 24581, \"det\": 44815, \"verb\": 50880, \"num\": 11905, \"adj\": 36440, \"noun\": 51202, \"adp\": 43937, \"adv\": 29599, \".\": 53331, \"prt\": 21888, \"pron\": 26974, \"x\": 2668}',\n",
              " 'index_docs': '{\"9\": 24581, \"5\": 44815, \"2\": 50880, \"11\": 11905, \"6\": 36440, \"1\": 51202, \"4\": 43937, \"7\": 29599, \"3\": 53331, \"10\": 21888, \"8\": 26974, \"12\": 2668}',\n",
              " 'index_word': '{\"1\": \"noun\", \"2\": \"verb\", \"3\": \".\", \"4\": \"adp\", \"5\": \"det\", \"6\": \"adj\", \"7\": \"adv\", \"8\": \"pron\", \"9\": \"conj\", \"10\": \"prt\", \"11\": \"num\", \"12\": \"x\"}',\n",
              " 'word_index': '{\"noun\": 1, \"verb\": 2, \".\": 3, \"adp\": 4, \"det\": 5, \"adj\": 6, \"adv\": 7, \"pron\": 8, \"conj\": 9, \"prt\": 10, \"num\": 11, \"x\": 12}'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwklrTqusjkL",
        "outputId": "0a8e59c7-1cc1-4f19-feeb-1aed5e92ed63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'noun': 1,\n",
              " 'verb': 2,\n",
              " '.': 3,\n",
              " 'adp': 4,\n",
              " 'det': 5,\n",
              " 'adj': 6,\n",
              " 'adv': 7,\n",
              " 'pron': 8,\n",
              " 'conj': 9,\n",
              " 'prt': 10,\n",
              " 'num': 11,\n",
              " 'x': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert text to number sequences by fitted tokenizer\n",
        "x_train_seqs = sentence_tokenizer.texts_to_sequences(x_train)"
      ],
      "metadata": {
        "id": "4vYvnVhTtmOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_seqs[0])\n",
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41PgV2eRt0BP",
        "outputId": "a42608eb-31ad-482c-d7a3-c17214164080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[133, 1921, 19, 8, 13, 1606, 461, 15, 4344, 318, 12, 8, 1922, 28157, 1858, 25, 12287, 28158, 926, 926, 20609, 7, 6603, 6, 20610, 1568, 9, 3198, 39, 13, 145, 6, 1742, 23, 834, 295, 3038, 311, 28159, 32, 28160, 1284, 3, 11, 207, 20609, 1742, 429, 2072, 12288, 15, 4]\n",
            "['Still', 'existing', 'on', 'a', '``', 'Northern', 'Union', \"''\", 'telegraph', 'form', 'is', 'a', 'typical', 'peremptory', 'message', 'from', 'Peru', 'grocer', 'J.', 'J.', 'Hapgood', 'to', 'Burton', 'and', \"Graves'\", 'store', 'in', 'Manchester', '--', '``', 'Get', 'and', 'send', 'by', 'stage', 'four', 'pounds', 'best', 'Porterhouse', 'or', 'serloin', 'stake', ',', 'for', 'Mrs.', 'Hapgood', 'send', 'six', 'sweet', 'oranges', \"''\", '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_seqs = tag_tokenizer.texts_to_sequences(y_train)"
      ],
      "metadata": {
        "id": "OIwxOqfdt4D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tag_tokenizer.sequences_to_texts([y_train_seqs[0]]))\n",
        "print(y_train_seqs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRyi-i-suIjq",
        "outputId": "a0ad622f-edc7-4e80-ea8f-8db4fba6d5bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['adv verb adp det . adj noun . noun noun verb det adj adj noun adp noun noun noun noun noun adp noun conj noun noun adp noun . . verb conj verb adp noun num noun adj noun conj noun noun . adp noun noun verb num adj noun . .']\n",
            "[7, 2, 4, 5, 3, 6, 1, 3, 1, 1, 2, 5, 6, 6, 1, 4, 1, 1, 1, 1, 1, 4, 1, 9, 1, 1, 4, 1, 3, 3, 2, 9, 2, 4, 1, 11, 1, 6, 1, 9, 1, 1, 3, 4, 1, 1, 2, 11, 6, 1, 3, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the same things to valid dataset\n",
        "x_val_seqs = sentence_tokenizer.texts_to_sequences(x_val)\n",
        "y_val_seqs = tag_tokenizer.texts_to_sequences(y_val)"
      ],
      "metadata": {
        "id": "7Vz817BkuSbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Even if RNN can handle variable lengthes of sequences, it is musch better for performance to unify the lengthes of each sequences\n",
        "print(len(max(x_train_seqs, key=len))) # return the length of the longest sequence\n",
        "MAX_LENGTH = len(max(x_train_seqs, key=len))\n",
        "print(f\"Length of longest input sequence: {MAX_LENGTH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5DE6_sOu4nV",
        "outputId": "aab55973-4d1c-4d50-9110-cac7804bc505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "271\n",
            "Length of longest input sequence: 271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can pad every sentences with method \"pad_sequences\" from keras\n",
        "x_train_padded = keras.preprocessing.sequence.pad_sequences(x_train_seqs, padding='post',\n",
        "                                                            maxlen=MAX_LENGTH)\n"
      ],
      "metadata": {
        "id": "WvjMFVPcviLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_padded[0])\n",
        "print(len(x_train_padded[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIC3PRHuy1JK",
        "outputId": "4394167d-dea2-4e10-fd54-db753a14a97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  133  1921    19     8    13  1606   461    15  4344   318    12     8\n",
            "  1922 28157  1858    25 12287 28158   926   926 20609     7  6603     6\n",
            " 20610  1568     9  3198    39    13   145     6  1742    23   834   295\n",
            "  3038   311 28159    32 28160  1284     3    11   207 20609  1742   429\n",
            "  2072 12288    15     4     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0]\n",
            "271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do the same things to training label\n",
        "y_train_padded = keras.preprocessing.sequence.pad_sequences(y_train_seqs, padding='post',\n",
        "                                                           maxlen=MAX_LENGTH)"
      ],
      "metadata": {
        "id": "PvY-cmPZy2zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val_padded = keras.preprocessing.sequence.pad_sequences(x_val_seqs, padding='post', maxlen=MAX_LENGTH)\n",
        "y_val_padded = keras.preprocessing.sequence.pad_sequences(y_val_seqs, padding='post', maxlen=MAX_LENGTH)"
      ],
      "metadata": {
        "id": "DTRgOeoXzHnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As PoS tagging is a multiclass classification task done at each timestep,\n",
        "# we need to convert every tag for every sentence into one-hot encoding.\n",
        "y_train_categoricals = keras.utils.to_categorical(y_train_padded)\n",
        "print(y_train_categoricals[0]) # sequence is now composed of one-hot encodings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlg3RsmtzUoN",
        "outputId": "cc08f9b4-84b0-4a8f-cab6-f7864f5f2c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encoding for a single tag in a sequence\n",
        "print(y_train_categoricals[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENGlUOJjzt3J",
        "outputId": "34bcecbb-5c71-49dc-f485-b57bd3d0be23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can determind PoS tag from oh-encoding by \"look-up\" in index_word dictionary from tag_tokenizer\n",
        "idx = np.argmax(y_train_categoricals[0][0]) # argmax return the index of elememt having maximum value in OHencoding array\n",
        "print(f\"Index: {idx}\")\n",
        "print(f\"Tag: {tag_tokenizer.index_word[idx]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsctW0Fm0gLH",
        "outputId": "2ba2f708-f72f-43b4-9d8c-f74c3ea9f135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 7\n",
            "Tag: adv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encoding to val_labels\n",
        "y_val_categoricals = keras.utils.to_categorical(y_val_padded)"
      ],
      "metadata": {
        "id": "Gu4HHrym1E4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[notes]\n",
        "At this point, we're ready to build our model. We'll train word embeddings concurrently with our model (though you can use pretrained word vectors as well)."
      ],
      "metadata": {
        "id": "CQ_OyvCSzyKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[notes]\n",
        "1. Ignore padding values :\n",
        "The embedding layers has *mask_zero* parameter. we added padding in order to make our batches the same size, but we don't want to makd PoS predictions on padding. Setting *mask_zero* to *True* makes the layers following the embedding layer ignore padding values.\n",
        "\n",
        "2. Return sequences not only one output :\n",
        "we're using *bidriectional LSTM*. The Bidrectional layer is a wrapper to which we pass an LSTM layer. The first parameter to the LSTM layer is the number of units in the cell. The second parameter, return_sequences, control whether the RNN returns an output for each timestep or only the last output. Since we're doing PoS-tagging, we want an aoutput for each timestep and so *return_sequences* is set to *True*."
      ],
      "metadata": {
        "id": "xepXLukU2YB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the embedding layer. \"+1\" to account for the padding token.\n",
        "num_tokens = len(sentence_tokenizer.word_index) + 1 # +1 for padding token\n",
        "embedding_dim = 128\n",
        "\n",
        "# For the output layer, The number of classes corresponds to the number of possible tags\n",
        "num_classes = len(tag_tokenizer.word_index) + 1 # also +1 for padding token"
      ],
      "metadata": {
        "id": "x63KC0tG4Oaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we set random_set_seed and kerner_initializer parameter to get same result.\n",
        "tf.random.set_seed(317)\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# input layer(embedding layer : each tokens -> embedding_dim )\n",
        "model.add(layers.Embedding(input_dim = num_tokens,\n",
        "                           output_dim = embedding_dim,\n",
        "                           input_length = MAX_LENGTH,\n",
        "                           mask_zero=True))\n",
        "\n",
        "# hidden layer (bidrectional)\n",
        "model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True,\n",
        "                                           kernel_initializer=tf.keras.initializers.random_normal(seed=317))))\n",
        "\n",
        "# output layer for each timestep with softmax activation fucntion\n",
        "model.add(layers.Dense(num_classes, activation='softmax',\n",
        "                       kernel_initializer=tf.keras.initializers.random_normal(seed=317)))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "CX8V0jM057Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kzrHF588OJi",
        "outputId": "bebe1f46-331c-4d47-cbb6-2ecd0f05ff67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 271, 128)          6679552   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 271, 256)         263168    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 271, 13)           3341      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,946,061\n",
            "Trainable params: 6,946,061\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[notes] \\\n",
        "1. The embedding layer output has three dimensions\n",
        "- Batch size : None => we haven't specified it yet\n",
        "- Sequence length : 217\n",
        "- Embedding dimension : 128\n",
        "\n",
        "2. Bidirectional LSTM outputs a vector twice the size of what we specified because its bidirectional. Remember two LSTM output will be concatenated before going to output layer.\n",
        "\n",
        "3. Output layer also has three dimensions\n",
        "- Batch size\n",
        "- Sequence length\n",
        "- Output dimension : 13 as number of tag classes\n"
      ],
      "metadata": {
        "id": "U0WjEnpT7w1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we put early-stopping for trainig to be stopped when validation loss stops improving.\n",
        "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history = model.fit(x_train_padded, y_train_categoricals, epochs=20,\n",
        "                    batch_size=256, validation_data=(x_val_padded, y_val_categoricals),\n",
        "                    callbacks=[es_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMSCaB708ZO1",
        "outputId": "8477a915-a062-4039-c8e4-eca7efeed24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "212/212 [==============================] - 65s 239ms/step - loss: 0.9598 - accuracy: 0.7057 - val_loss: 0.1617 - val_accuracy: 0.9512\n",
            "Epoch 2/20\n",
            "212/212 [==============================] - 25s 117ms/step - loss: 0.1138 - accuracy: 0.9642 - val_loss: 0.1082 - val_accuracy: 0.9642\n",
            "Epoch 3/20\n",
            "212/212 [==============================] - 17s 81ms/step - loss: 0.0753 - accuracy: 0.9753 - val_loss: 0.0968 - val_accuracy: 0.9678\n",
            "Epoch 4/20\n",
            "212/212 [==============================] - 20s 94ms/step - loss: 0.0595 - accuracy: 0.9803 - val_loss: 0.0944 - val_accuracy: 0.9692\n",
            "Epoch 5/20\n",
            "212/212 [==============================] - 14s 66ms/step - loss: 0.0480 - accuracy: 0.9842 - val_loss: 0.0924 - val_accuracy: 0.9703\n",
            "Epoch 6/20\n",
            "212/212 [==============================] - 12s 57ms/step - loss: 0.0392 - accuracy: 0.9875 - val_loss: 0.0954 - val_accuracy: 0.9698\n",
            "Epoch 7/20\n",
            "212/212 [==============================] - 12s 55ms/step - loss: 0.0322 - accuracy: 0.9898 - val_loss: 0.0995 - val_accuracy: 0.9701\n",
            "Epoch 8/20\n",
            "212/212 [==============================] - 12s 55ms/step - loss: 0.0265 - accuracy: 0.9918 - val_loss: 0.1026 - val_accuracy: 0.9700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After our model is trained, go to test data set\n",
        "# preprocessing(tokenize, pad) it and oh encoding\n",
        "\n",
        "x_test_seqs = sentence_tokenizer.texts_to_sequences(x_test)\n",
        "x_test_padded = keras.preprocessing.sequence.pad_sequences(x_test_seqs, padding='post', maxlen=MAX_LENGTH)\n",
        "\n",
        "y_test_seqs = tag_tokenizer.texts_to_sequences(y_test)\n",
        "y_test_padded = keras.preprocessing.sequence.pad_sequences(y_test_seqs, padding='post', maxlen=MAX_LENGTH)\n",
        "y_test_categoricals = keras.utils.to_categorical(y_test_padded)"
      ],
      "metadata": {
        "id": "QqxYjmQn9ve9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test_padded, y_test_categoricals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n4n9Jly-ruD",
        "outputId": "2607734f-36d7-4a4a-cc63-98522d620bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226/226 [==============================] - 2s 10ms/step - loss: 0.1052 - accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1051989421248436, 0.9687950611114502]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can use our models to tag sentences.\n",
        "\n",
        "samples = [\n",
        "    \"Brown refused to testify.\",\n",
        "    \"Brown sofas are on sale\",\n",
        "]"
      ],
      "metadata": {
        "id": "_Ac9odm9-wJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# develop simple function for doing this task.\n",
        "\n",
        "def tag_sentences(sentences):\n",
        "  sentences_seqs = sentence_tokenizer.texts_to_sequences(sentences)\n",
        "  sentences_padded = keras.preprocessing.sequence.pad_sequences(sentences_seqs,\n",
        "                                                               maxlen=MAX_LENGTH,\n",
        "                                                               padding='post')\n",
        "  # tag_preds is each list of probabilty distribution (softmax)\n",
        "  tag_preds = model.predict(sentences_padded)\n",
        "\n",
        "  sentence_tags = []\n",
        "\n",
        "  # each iteration is one sequence\n",
        "  for i, preds in enumerate(tag_preds):\n",
        "\n",
        "    print(preds)\n",
        "    # seq of most probable ones in sequence\n",
        "    tags_seq = [np.argmax(p) for p in preds[:len(sentences_seqs[i])]]\n",
        "    words = [sentence_tokenizer.index_word[w] for w in sentences_seqs[i]]\n",
        "    tags = [tag_tokenizer.index_word[t] for t in tags_seq]\n",
        "    sentence_tags.append(list(zip(words, tags)))\n",
        "\n",
        "  return sentence_tags"
      ],
      "metadata": {
        "id": "fjDS3PHZ_DHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_sample_sentences = tag_sentences(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot1Wu41vAKZ2",
        "outputId": "766dfe38-e775-420f-c057-e8fa0e7b94a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "[[3.2334467e-06 9.9331737e-01 2.9946794e-04 ... 3.9344709e-04\n",
            "  1.2747691e-05 2.2187696e-05]\n",
            " [1.2938746e-06 3.1313516e-04 9.9806172e-01 ... 1.4160351e-04\n",
            "  4.5686566e-09 1.3718624e-06]\n",
            " [1.2729881e-06 2.0816846e-05 6.1781978e-04 ... 4.8411652e-01\n",
            "  3.2293276e-05 3.9092065e-06]\n",
            " ...\n",
            " [7.5027689e-02 8.0436386e-02 7.9266421e-02 ... 7.5670168e-02\n",
            "  7.5860150e-02 7.5894080e-02]\n",
            " [7.5027689e-02 8.0436386e-02 7.9266421e-02 ... 7.5670168e-02\n",
            "  7.5860150e-02 7.5894080e-02]\n",
            " [7.5027689e-02 8.0436386e-02 7.9266421e-02 ... 7.5670168e-02\n",
            "  7.5860150e-02 7.5894080e-02]]\n",
            "[[4.2764573e-06 1.5268865e-01 1.3265687e-04 ... 3.5671474e-05\n",
            "  2.9227600e-04 1.2731762e-05]\n",
            " [2.3185294e-08 9.9997950e-01 6.1821049e-07 ... 1.7081586e-06\n",
            "  1.0583433e-06 1.6246985e-06]\n",
            " [4.9634735e-10 3.8132605e-06 9.9999332e-01 ... 1.6310411e-08\n",
            "  2.7530690e-13 1.3395328e-08]\n",
            " ...\n",
            " [7.5027689e-02 8.0436386e-02 7.9266421e-02 ... 7.5670168e-02\n",
            "  7.5860150e-02 7.5894080e-02]\n",
            " [7.5027689e-02 8.0436386e-02 7.9266421e-02 ... 7.5670168e-02\n",
            "  7.5860150e-02 7.5894080e-02]\n",
            " [7.5027689e-02 8.0436386e-02 7.9266421e-02 ... 7.5670168e-02\n",
            "  7.5860150e-02 7.5894080e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tagged_sample_sentences[0])\n",
        "print(tagged_sample_sentences[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRumHAfjAOcF",
        "outputId": "b1088276-0b8c-4334-e981-9d76f27ac458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('brown', 'noun'), ('refused', 'verb'), ('to', 'adp'), ('testify', 'verb')]\n",
            "[('brown', 'adj'), ('sofas', 'noun'), ('are', 'verb'), ('on', 'adp'), ('sale', 'noun')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's just one way of buidling a PoS tagger, these days' PoS tagger is much more sophisticated models in which transfomer is used."
      ],
      "metadata": {
        "id": "sYL-XcXyBwN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Modelling With Stacked LSTMs"
      ],
      "metadata": {
        "id": "7aNbJ1L77w3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download sample corpus\n",
        "art_of_war = requests.get('https://raw.githubusercontent.com/nitinpunjabi/nlp-demystified/main/datasets/art_of_war.txt')\\\n",
        "                     .text\n",
        "\n",
        "art_of_war[:300]"
      ],
      "metadata": {
        "id": "yn1fuCFAB6fo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "37f4c44f-a0af-45c3-ef67-ab43a3af89c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1. Sun Tzŭ said: The art of war is of vital importance to the State.\\n\\n2. It is a matter of life and death, a road either to safety or to\\nruin. Hence it is a subject of inquiry which can on no account be\\nneglected.\\n\\n3. The art of war, then, is governed by five constant factors, to be\\ntaken into accou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[notes]\\\n",
        "We will build character-based language model(opposed to word-based)\n",
        "\n",
        "Character-level models have the advantage of:\n",
        "1. Smaller prediction space (smaller character than words.. in english obviously)\n",
        "2. More resilient to the out-of-vocabulary(OOV) problem. and better able to learn lower mechanics of language (including punctuation)\n",
        "\n",
        "On the other hand, character-level models need to learn a sequence of charcters to \"make sense\" of a word (e.g. the sequence of \"c\", \"a\", \"t\" to identify \"cat\" as a pattern) which can be inefficient and result in lower performance.\n",
        "\n",
        "RNNs can process any kind of sequence so what's shown here can easily be applied at the word level. When we cover transformers, there is an alterniative approaches called subword tokenization which is the moddle-ground between these two approaches.\n"
      ],
      "metadata": {
        "id": "mksVVK2b8db_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level = True)"
      ],
      "metadata": {
        "id": "IJvSKHAj8STw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([art_of_war])"
      ],
      "metadata": {
        "id": "UfJi-2Gd91V8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_config()"
      ],
      "metadata": {
        "id": "mBOBdC3r97yF",
        "outputId": "31b93c6b-8d02-4059-f1fd-591f8ee36444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_words': None,\n",
              " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
              " 'lower': True,\n",
              " 'split': ' ',\n",
              " 'char_level': True,\n",
              " 'oov_token': None,\n",
              " 'document_count': 1,\n",
              " 'word_counts': '{\"1\": 179, \".\": 896, \" \": 9794, \"s\": 3081, \"u\": 1467, \"n\": 3565, \"t\": 4398, \"z\": 20, \"\\\\u016d\": 13, \"a\": 3475, \"i\": 3573, \"d\": 1681, \":\": 48, \"h\": 2558, \"e\": 5837, \"r\": 2776, \"o\": 3548, \"f\": 1238, \"w\": 981, \"v\": 478, \"l\": 1722, \"m\": 1201, \"p\": 769, \"c\": 1390, \"\\\\n\": 1443, \"2\": 127, \",\": 634, \"y\": 1055, \"b\": 708, \"j\": 23, \"q\": 55, \"g\": 1007, \"3\": 87, \"k\": 345, \"\\\\u2019\": 57, \"4\": 66, \"(\": 59, \")\": 59, \";\": 168, \"5\": 58, \"6\": 51, \"_\": 62, \"7\": 39, \"8\": 36, \"9\": 34, \"0\": 38, \"x\": 49, \"\\\\u2014\": 16, \"?\": 8, \"!\": 8, \"-\": 57, \"\\\\u201c\": 3, \"\\\\u201d\": 3, \"\\\\u0153\": 7, \"\\\\u00fc\": 3, \"\\\\u2018\": 1}',\n",
              " 'word_docs': '{\"(\": 1, \"l\": 1, \"s\": 1, \"5\": 1, \"m\": 1, \"!\": 1, \"7\": 1, \"3\": 1, \"j\": 1, \"?\": 1, \"c\": 1, \"k\": 1, \".\": 1, \"\\\\u2019\": 1, \"h\": 1, \"u\": 1, \"r\": 1, \"1\": 1, \"\\\\u201d\": 1, \"i\": 1, \"o\": 1, \"\\\\n\": 1, \"g\": 1, \"a\": 1, \"x\": 1, \"b\": 1, \"e\": 1, \"w\": 1, \",\": 1, \"n\": 1, \" \": 1, \"\\\\u00fc\": 1, \";\": 1, \"f\": 1, \"9\": 1, \"d\": 1, \"t\": 1, \":\": 1, \"-\": 1, \"v\": 1, \"6\": 1, \"0\": 1, \"\\\\u201c\": 1, \"_\": 1, \")\": 1, \"\\\\u016d\": 1, \"\\\\u2014\": 1, \"2\": 1, \"4\": 1, \"q\": 1, \"\\\\u0153\": 1, \"y\": 1, \"8\": 1, \"\\\\u2018\": 1, \"z\": 1, \"p\": 1}',\n",
              " 'index_docs': '{\"33\": 1, \"11\": 1, \"8\": 1, \"35\": 1, \"17\": 1, \"51\": 1, \"42\": 1, \"30\": 1, \"46\": 1, \"50\": 1, \"15\": 1, \"26\": 1, \"21\": 1, \"36\": 1, \"10\": 1, \"13\": 1, \"9\": 1, \"27\": 1, \"54\": 1, \"4\": 1, \"6\": 1, \"14\": 1, \"19\": 1, \"7\": 1, \"40\": 1, \"23\": 1, \"2\": 1, \"20\": 1, \"24\": 1, \"5\": 1, \"1\": 1, \"55\": 1, \"28\": 1, \"16\": 1, \"45\": 1, \"12\": 1, \"3\": 1, \"41\": 1, \"37\": 1, \"25\": 1, \"39\": 1, \"43\": 1, \"53\": 1, \"32\": 1, \"34\": 1, \"49\": 1, \"48\": 1, \"29\": 1, \"31\": 1, \"38\": 1, \"52\": 1, \"18\": 1, \"44\": 1, \"56\": 1, \"47\": 1, \"22\": 1}',\n",
              " 'index_word': '{\"1\": \" \", \"2\": \"e\", \"3\": \"t\", \"4\": \"i\", \"5\": \"n\", \"6\": \"o\", \"7\": \"a\", \"8\": \"s\", \"9\": \"r\", \"10\": \"h\", \"11\": \"l\", \"12\": \"d\", \"13\": \"u\", \"14\": \"\\\\n\", \"15\": \"c\", \"16\": \"f\", \"17\": \"m\", \"18\": \"y\", \"19\": \"g\", \"20\": \"w\", \"21\": \".\", \"22\": \"p\", \"23\": \"b\", \"24\": \",\", \"25\": \"v\", \"26\": \"k\", \"27\": \"1\", \"28\": \";\", \"29\": \"2\", \"30\": \"3\", \"31\": \"4\", \"32\": \"_\", \"33\": \"(\", \"34\": \")\", \"35\": \"5\", \"36\": \"\\\\u2019\", \"37\": \"-\", \"38\": \"q\", \"39\": \"6\", \"40\": \"x\", \"41\": \":\", \"42\": \"7\", \"43\": \"0\", \"44\": \"8\", \"45\": \"9\", \"46\": \"j\", \"47\": \"z\", \"48\": \"\\\\u2014\", \"49\": \"\\\\u016d\", \"50\": \"?\", \"51\": \"!\", \"52\": \"\\\\u0153\", \"53\": \"\\\\u201c\", \"54\": \"\\\\u201d\", \"55\": \"\\\\u00fc\", \"56\": \"\\\\u2018\"}',\n",
              " 'word_index': '{\" \": 1, \"e\": 2, \"t\": 3, \"i\": 4, \"n\": 5, \"o\": 6, \"a\": 7, \"s\": 8, \"r\": 9, \"h\": 10, \"l\": 11, \"d\": 12, \"u\": 13, \"\\\\n\": 14, \"c\": 15, \"f\": 16, \"m\": 17, \"y\": 18, \"g\": 19, \"w\": 20, \".\": 21, \"p\": 22, \"b\": 23, \",\": 24, \"v\": 25, \"k\": 26, \"1\": 27, \";\": 28, \"2\": 29, \"3\": 30, \"4\": 31, \"_\": 32, \"(\": 33, \")\": 34, \"5\": 35, \"\\\\u2019\": 36, \"-\": 37, \"q\": 38, \"6\": 39, \"x\": 40, \":\": 41, \"7\": 42, \"0\": 43, \"8\": 44, \"9\": 45, \"j\": 46, \"z\": 47, \"\\\\u2014\": 48, \"\\\\u016d\": 49, \"?\": 50, \"!\": 51, \"\\\\u0153\": 52, \"\\\\u201c\": 53, \"\\\\u201d\": 54, \"\\\\u00fc\": 55, \"\\\\u2018\": 56}'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tokenizer \\\"Vocabulary\\\" size : {len(tokenizer.word_index)}\")"
      ],
      "metadata": {
        "id": "ZYrlMioO9_Cn",
        "outputId": "117976b5-91b1-48e8-8097-d8a9f2c6c710",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer \"Vocabulary\" size : 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq = tokenizer.texts_to_sequences([art_of_war])[0] # first sequence"
      ],
      "metadata": {
        "id": "9SsU36W4-WdL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Text length: {len(seq)}\")"
      ],
      "metadata": {
        "id": "063vMdCZ-mIq",
        "outputId": "d3e6381a-9717-4992-d2ae-07a932ea5c6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text length: 61054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check.\n",
        "tokenizer.sequences_to_texts([seq[:10]])"
      ],
      "metadata": {
        "id": "W36mbIrV-ptA",
        "outputId": "d42f650e-f9db-4f7d-c891-7bdaff2a7081",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1 .   s u n   t z ŭ']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll gonna use \"Tensorflow Data API\" which makes it easy to build preprocessing pipelines by chaining operations together\n",
        "# To use this, we need to converted our vectorized corpus into Dataset object (from_tensor_slices)\n",
        "slices = tf.data.Dataset.from_tensor_slices(seq)\n",
        "type(slices)"
      ],
      "metadata": {
        "id": "A_x1HfLe-6Hk",
        "outputId": "a533b669-d985-49e4-b381-8bfd4beba7fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.from_tensor_slices_op._TensorSliceDataset"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(slices.take(10))"
      ],
      "metadata": {
        "id": "idGOsXMn_f5a",
        "outputId": "18fbce4e-b955-40fd-924c-ed4b71ffdb92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=int32, numpy=27>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=21>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=8>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=13>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=5>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=3>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=47>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=49>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq[:10]"
      ],
      "metadata": {
        "id": "ah1LF2Du_kiq",
        "outputId": "f62ae9ee-5126-427f-c1bf-519ab2ca357a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[27, 21, 1, 8, 13, 5, 1, 3, 47, 49]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll use window method. window size will be 100. Dataset of 1000 willbe sequence of 10 datasets, each containing 100 elements\n",
        "# Here, we create window \"input_timesteps + 1\" which is for target/label for each training example.\n",
        "# Shift to 1, window shift tactics.\n",
        "# Drop_remainder to True which ensure All windows contain exactly N elements. i.e. once input contians fewer than N, it dropped.\n",
        "input_timesteps = 100\n",
        "window_size = input_timesteps + 1\n",
        "windows = slices.window(window_size, shift=1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "meDEWgsW_mjg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in windows.take(3):\n",
        "  arr = list(w.as_numpy_iterator())\n",
        "  print(len(arr), arr)"
      ],
      "metadata": {
        "id": "JHlKluGpA0yi",
        "outputId": "139733ca-e7c4-47f8-ddd9-c65bc4fe65a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 [27, 21, 1, 8, 13, 5, 1, 3, 47, 49, 1, 8, 7, 4, 12, 41, 1, 3, 10, 2, 1, 7, 9, 3, 1, 6, 16, 1, 20, 7, 9, 1, 4, 8, 1, 6, 16, 1, 25, 4, 3, 7, 11, 1, 4, 17, 22, 6, 9, 3, 7, 5, 15, 2, 1, 3, 6, 1, 3, 10, 2, 1, 8, 3, 7, 3, 2, 21, 14, 14, 29, 21, 1, 4, 3, 1, 4, 8, 1, 7, 1, 17, 7, 3, 3, 2, 9, 1, 6, 16, 1, 11, 4, 16, 2, 1, 7, 5, 12, 1, 12]\n",
            "101 [21, 1, 8, 13, 5, 1, 3, 47, 49, 1, 8, 7, 4, 12, 41, 1, 3, 10, 2, 1, 7, 9, 3, 1, 6, 16, 1, 20, 7, 9, 1, 4, 8, 1, 6, 16, 1, 25, 4, 3, 7, 11, 1, 4, 17, 22, 6, 9, 3, 7, 5, 15, 2, 1, 3, 6, 1, 3, 10, 2, 1, 8, 3, 7, 3, 2, 21, 14, 14, 29, 21, 1, 4, 3, 1, 4, 8, 1, 7, 1, 17, 7, 3, 3, 2, 9, 1, 6, 16, 1, 11, 4, 16, 2, 1, 7, 5, 12, 1, 12, 2]\n",
            "101 [1, 8, 13, 5, 1, 3, 47, 49, 1, 8, 7, 4, 12, 41, 1, 3, 10, 2, 1, 7, 9, 3, 1, 6, 16, 1, 20, 7, 9, 1, 4, 8, 1, 6, 16, 1, 25, 4, 3, 7, 11, 1, 4, 17, 22, 6, 9, 3, 7, 5, 15, 2, 1, 3, 6, 1, 3, 10, 2, 1, 8, 3, 7, 3, 2, 21, 14, 14, 29, 21, 1, 4, 3, 1, 4, 8, 1, 7, 1, 17, 7, 3, 3, 2, 9, 1, 6, 16, 1, 11, 4, 16, 2, 1, 7, 5, 12, 1, 12, 2, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# window method returns a nested dataset of datasets\n",
        "print(windows, '\\n')\n",
        "\n",
        "for w in windows.take(2):\n",
        "  print(w)"
      ],
      "metadata": {
        "id": "JJgkRDvSBBIx",
        "outputId": "558ca861-ccdc-4e26-aa9b-c3da1761ad30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_WindowDataset element_spec=DatasetSpec(TensorSpec(shape=(), dtype=tf.int32, name=None), TensorShape([]))> \n",
            "\n",
            "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
            "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our model only accept tensors. We need to extract tensors from each window using 'flat_map'\n",
        "dataset = windows.flat_map(lambda window: window.batch(window_size))"
      ],
      "metadata": {
        "id": "TxCZ1fTRBO4T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we have single dataset of tensors, where each tensor is \"input_timesteps+1\" long and shifted by 1.\n",
        "for d in dataset.take(2):\n",
        "  print(d)"
      ],
      "metadata": {
        "id": "Y60dbqMXBwax",
        "outputId": "b977037a-c68f-4072-c8f8-129fdbcd02aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[27 21  1  8 13  5  1  3 47 49  1  8  7  4 12 41  1  3 10  2  1  7  9  3\n",
            "  1  6 16  1 20  7  9  1  4  8  1  6 16  1 25  4  3  7 11  1  4 17 22  6\n",
            "  9  3  7  5 15  2  1  3  6  1  3 10  2  1  8  3  7  3  2 21 14 14 29 21\n",
            "  1  4  3  1  4  8  1  7  1 17  7  3  3  2  9  1  6 16  1 11  4 16  2  1\n",
            "  7  5 12  1 12], shape=(101,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[21  1  8 13  5  1  3 47 49  1  8  7  4 12 41  1  3 10  2  1  7  9  3  1\n",
            "  6 16  1 20  7  9  1  4  8  1  6 16  1 25  4  3  7 11  1  4 17 22  6  9\n",
            "  3  7  5 15  2  1  3  6  1  3 10  2  1  8  3  7  3  2 21 14 14 29 21  1\n",
            "  4  3  1  4  8  1  7  1 17  7  3  3  2  9  1  6 16  1 11  4 16  2  1  7\n",
            "  5 12  1 12  2], shape=(101,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create batches from our data set. shuffle and create!\n",
        "batch_size = 32\n",
        "batches = dataset.shuffle(10000).batch(batch_size)"
      ],
      "metadata": {
        "id": "Tqsm3wwXB_Yq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b in batches.take(2):\n",
        "  print(b)"
      ],
      "metadata": {
        "id": "AB_odxN3CLnu",
        "outputId": "2b31238a-515c-4928-d341-97f04af4ecfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 5 12  1 ...  5 12  1]\n",
            " [ 1  9  2 ...  1  3  6]\n",
            " [ 2  1  7 ...  4 11 25]\n",
            " ...\n",
            " [ 9  2  3 ... 34  1 23]\n",
            " [16  9  6 ...  7  9  4]\n",
            " [ 1  8  3 ... 18  6 13]], shape=(32, 101), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[ 3  7  5 ...  5  1  3]\n",
            " [ 2  1  7 ...  1  7  5]\n",
            " [ 9  8 24 ...  2  5 12]\n",
            " ...\n",
            " [22  9  2 ... 14 14 30]\n",
            " [ 5  1  2 ...  9 15  2]\n",
            " [ 1 17  7 ... 10  1  3]], shape=(32, 101), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[notes]\\\n",
        "We talked about _Teacher Forcing_\n",
        "1. At each timestep during training, the output is compared to a label\n",
        "2. At the next timestep, rather than feeding the model the previous output, we feed it the next character of input sequence(i.e. what model should've outputted - label!)\n",
        "\n",
        "This is why each sequence is of size _input_timestep+1_. Each sequence is now going to be separated into Two sequences. The first sequence will be training input and will be length input_timepsteps. The second sequence will be the label/target and will consist of all the sequence elements shifted by 1.\n",
        "\n",
        "So if processed sequence is \"she swam in the lake\", then:\n",
        "+ the input will be \"she swam in the lak\" (drop last char)\n",
        "+ the target/label will be \"he swam in the lake\" (drop first char)\n"
      ],
      "metadata": {
        "id": "N2lBrbpxCVpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x-y paired bathces\n",
        "xy_batches = batches.map(lambda batch: (batch[:, :-1], batch[:, 1:]))"
      ],
      "metadata": {
        "id": "SbbF6rlsCODz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b in xy_batches.take(1):\n",
        "  print(b)"
      ],
      "metadata": {
        "id": "T3BgobHRELlG",
        "outputId": "bd0f572f-7f93-442c-f4d6-c3780eb238f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
            "array([[ 2, 17,  8, ...,  8,  7,  5],\n",
            "       [ 9,  2,  3, ..., 29, 34,  1],\n",
            "       [ 1, 20, 10, ...,  1, 20,  7],\n",
            "       ...,\n",
            "       [12,  8,  1, ...,  6,  3,  1],\n",
            "       [50, 14, 33, ...,  1, 17,  6],\n",
            "       [15,  2,  1, ..., 13,  7,  4]], dtype=int32)>, <tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
            "array([[17,  8,  1, ...,  7,  5, 12],\n",
            "       [ 2,  3,  9, ..., 34,  1, 23],\n",
            "       [20, 10,  6, ..., 20,  7, 18],\n",
            "       ...,\n",
            "       [ 8,  1,  8, ...,  3,  1, 20],\n",
            "       [14, 33, 30, ..., 17,  6,  8],\n",
            "       [ 2,  1,  6, ...,  7,  4,  5]], dtype=int32)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For greater clarity, this is first input sequence from the first batch, and its's corresponding label/target sequence.\n",
        "for b in xy_batches.take(1): # first batch\n",
        "  print(\"x1 length: \", len(b[0][0].numpy())) # first sequence (X)\n",
        "  print(\"x1: \", b[0][0].numpy())\n",
        "\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"y1 length: \", len(b[1][0].numpy())) # first sequence (X)\n",
        "  print(\"y1: \", b[1][0].numpy())"
      ],
      "metadata": {
        "id": "qMnsCRHGEXp7",
        "outputId": "bce3c58b-2a0d-4bde-c993-1fdbba94e880",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1 length:  100\n",
            "x1:  [ 1  3  6  1 25  4 15  3  6  9 18 24  1 17 13  8  3  1  5  6  3  1 23  2\n",
            "  1 12  4 25 13 11 19  2 12 14 23  2 16  6  9  2 10  7  5 12 21 14 14 29\n",
            " 39 21  1  5  6 20  1  3 10  2  1 19  2  5  2  9  7 11  1 20 10  6  1 20\n",
            "  4  5  8  1  7  1 23  7  3  3 11  2  1 17  7 26  2  8  1 17  7  5 18  1\n",
            " 15  7 11 15]\n",
            "\n",
            "\n",
            "y1 length:  100\n",
            "y1:  [ 3  6  1 25  4 15  3  6  9 18 24  1 17 13  8  3  1  5  6  3  1 23  2  1\n",
            " 12  4 25 13 11 19  2 12 14 23  2 16  6  9  2 10  7  5 12 21 14 14 29 39\n",
            " 21  1  5  6 20  1  3 10  2  1 19  2  5  2  9  7 11  1 20 10  6  1 20  4\n",
            "  5  8  1  7  1 23  7  3  3 11  2  1 17  7 26  2  8  1 17  7  5 18  1 15\n",
            "  7 11 15 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[notes] \\\n",
        "Last step is one-hot encode the inputs. because\n",
        "1. We're not using embeddings for the input. We can, but since this is a charcter model with just a few dozen possible choices, we can get away with one-hot encoding. There's also no reason to think a particular letter should be closer to another in vector space as we would want in a word-level model\n",
        "2. Since we're not using embeddings and our input is categorical, we need to one-hot encode\n",
        "\n",
        "Also, NOT one-hot encode label/target data.\n",
        "This is becuase we'll be using a loss function that can help us skip that step (more below)"
      ],
      "metadata": {
        "id": "0f7yJ762JoVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(tokenizer.word_index) + 1\n",
        "\n",
        "# One-hot encode the input sequences, don't do anything with the label/target sequences\n",
        "xy_batches = xy_batches.map(lambda inputs, labels: (tf.one_hot(inputs, depth=num_tokens), labels))"
      ],
      "metadata": {
        "id": "HUDZO-2WEn8x"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each input sequence is now a sequence of one-hot encodings\n",
        "for b in xy_batches.take(1):\n",
        "  print(\"x1 \", b[0][0].numpy())\n",
        "  print(\"\\n\")\n",
        "  print(\"y1 \", b[1][0].numpy())\n"
      ],
      "metadata": {
        "id": "lafQCyu-LLg-",
        "outputId": "09596070-39f6-4478-8026-7ec847a73a62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1  [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "\n",
            "y1  [ 2  1 23 13 11 20  7  9 26  1  4  8 14 15  6 17 22 11  2  3  2  1  7  3\n",
            "  1  7 11 11  1 22  6  4  5  3  8 28  1  3 10  2  1  8  3  7  3  2  1 20\n",
            "  4 11 11  1 23  2  1  8  3  9  6  5 19 28  1  4 16  1  3 10  2  1 23 13\n",
            " 11 20  7  9 26  1  4  8 14 12  2 16  2 15  3  4 25  2 24  1  3 10  2  1\n",
            "  8  3  7  3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[note] \\\n",
        "At this point, we've\n",
        "+ Segmented our corpus into fix-ed length sequences\n",
        "+ Created training and label/target sequences\n",
        "+ Organized them into batches.\n",
        "\n",
        "We need to add \"prefetching\" which is optimizing step.\n",
        "This way, while the model trains on the current batch of data, the pipeline read and prepares the next batch.\n"
      ],
      "metadata": {
        "id": "bfcAkjsIL1Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "_1VTdrXzLdfD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[note] \\\n",
        "Now build our model\n",
        "Three new things to remember\n",
        "1. We're stacking two LSTMs, the sequential output of the first LSTM will become the sequential input to the second LSTM\n",
        "2. We're adding some _recurrent_dropout_. This drops connections between the recurrent units(i.e. the dropout is applied horizontally across time). You can still use regular dropout as well which will be applied to the inputs/outputs.\n",
        "3. We're using __sparse_categorical_crossentropy__. This allows us to provide labels as integers for multiclass classification rather than one-hot encodings"
      ],
      "metadata": {
        "id": "EqG3v2GpMRsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(layers.LSTM(128, return_sequences=True, input_shape=[None, num_tokens], recurrent_dropout=0.2))\n",
        "model.add(layers.LSTM(128, return_sequences=True, input_shape=[None, num_tokens], recurrent_dropout=0.2))\n",
        "\n",
        "model.add(layers.Dense(num_tokens, activation='softmax'))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam')"
      ],
      "metadata": {
        "id": "cREgRhtCMQAP",
        "outputId": "0d314a42-320a-4549-df9b-a8065696fffc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "zz_Vb3OmNRhM",
        "outputId": "262f60bc-e981-4acb-f52e-9d42904611e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, None, 128)         95232     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, None, 128)         131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 57)          7353      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 234,169\n",
            "Trainable params: 234,169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This model takes a few hours to train, we're using model checkpoints to save the weights after every epoch.\n",
        "# If something goes wrong with our system during training, we can reload the last set of weights from the checkpoint.\n",
        "filepath=\"./ArtofWarLM/training1/cp.ckpt\"\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
        "                                                 save_seights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "metadata": {
        "id": "n58IfXzdNU3n"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when call model's fit method,\n",
        "# Simply pass in the batches as is (no need to separate into explicit x and y arguments)\n",
        "# pass the model checkpoint callback to save weights after every epoch\n",
        "# [alert!] fit commented out becuase this model takes a few hours to train, I trained it ahead of time and saved it\n",
        "#\n",
        "# history = model.fit(xy_batches, epochs = 50, callback=[cp_callback])\n",
        "#"
      ],
      "metadata": {
        "id": "KZQ3IT-zNylE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Once the model training is complete, we can save this weight using model's save\n",
        "#\n",
        "# model.save('art_of_war_char_level_lm')\n",
        "#"
      ],
      "metadata": {
        "id": "ZkqoZ3_ROdwl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the pretrained model..\n",
        "!wget https://github.com/nitinpunjabi/nlp-demystified/raw/main/models/art_of_war_char_level_lm.zip\n",
        "!unzip -o art_of_war_char_level_lm.zip"
      ],
      "metadata": {
        "id": "9IMsXiHxOrms",
        "outputId": "02c768b8-6cdc-4cea-d489-4f65d413483d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-11 15:33:49--  https://github.com/nitinpunjabi/nlp-demystified/raw/main/models/art_of_war_char_level_lm.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nitinpunjabi/nlp-demystified/main/models/art_of_war_char_level_lm.zip [following]\n",
            "--2023-08-11 15:33:50--  https://raw.githubusercontent.com/nitinpunjabi/nlp-demystified/main/models/art_of_war_char_level_lm.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2691531 (2.6M) [application/zip]\n",
            "Saving to: ‘art_of_war_char_level_lm.zip’\n",
            "\n",
            "art_of_war_char_lev 100%[===================>]   2.57M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-08-11 15:33:50 (207 MB/s) - ‘art_of_war_char_level_lm.zip’ saved [2691531/2691531]\n",
            "\n",
            "Archive:  art_of_war_char_level_lm.zip\n",
            "   creating: art_of_war_char_level_lm/\n",
            "   creating: art_of_war_char_level_lm/assets/\n",
            "   creating: art_of_war_char_level_lm/variables/\n",
            "  inflating: art_of_war_char_level_lm/variables/variables.data-00000-of-00001  \n",
            "  inflating: art_of_war_char_level_lm/variables/variables.index  \n",
            "  inflating: art_of_war_char_level_lm/saved_model.pb  \n",
            "  inflating: art_of_war_char_level_lm/keras_metadata.pb  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load\n",
        "model = keras.models.load_model('art_of_war_char_level_lm')"
      ],
      "metadata": {
        "id": "7S4m6zaWO34A",
        "outputId": "5f0d2d31-2c3e-4bfe-dcff-fabb168783df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[note] \\\n",
        "Now that we have a trained model, let's generate some text.\n",
        "\n",
        "The function below takes some seed text and uses that to generate a certain number of characters. For each character, it uses the generated text so far as the input. It's not the most efficient function but it'll work here.\n",
        "\n",
        "There's also a temperature parameter. The next character is picked from a probability distribution. By dividing the log of this distribution by temperature, we can influence the randomness of the output.\n",
        "\n",
        "When the temperature is low (< 1), the probability distribution sharpens and the model will be more strict in recreating the original text. As we raise the temperature, the distribution flattens and there's a higher chance the model picks something unexpected, resulting in greater surprise in the output. In practice, a high enough temperature will result in nonsense."
      ],
      "metadata": {
        "id": "3adDcr2IPNfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_timesteps)"
      ],
      "metadata": {
        "id": "i84GGa2IQoDT",
        "outputId": "0b219738-752a-45fc-de7a-50d6297e50f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seed_text, num_chars=200, temperature=1):\n",
        "\n",
        "  text = seed_text\n",
        "\n",
        "  for _ in range(num_chars):\n",
        "\n",
        "    # Take the last *input_timesteps* number of characters in the text so far\n",
        "    # as input.\n",
        "    input = np.array(tokenizer.texts_to_sequences([text[-input_timesteps:]]))\n",
        "    input = tf.one_hot(input, num_tokens)\n",
        "\n",
        "    # Create probability distribution for next character adjusted by temperature.\n",
        "    preds = model.predict(input)[0, -1:, :] # <-- We want only the last character so we're extracting the softmax output for that.\n",
        "    preds = tf.math.log(preds) / temperature\n",
        "\n",
        "    # Sample next character and add to running text.\n",
        "    next_char = tf.random.categorical(preds, num_samples=1)\n",
        "    next_char = tokenizer.sequences_to_texts(next_char.numpy())[0]\n",
        "\n",
        "    text += next_char\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "yaLYDwvjO9NW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(generate_text(model, tokenizer, \"Banana peels on the battlefield can\", num_chars=300, temperature=0.2))"
      ],
      "metadata": {
        "id": "mCIOQmEYRgQN",
        "outputId": "79fc0069-5eb8-48e7-e0d4-2ac4c2e854b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 452ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "Banana peels on the battlefield can never come again\n",
            "into being; nor can the dead ever be brought back to life.\n",
            "\n",
            "22. hence the enlightened ruler lays his plans well ahead;\n",
            "the good general cultivates his resources.\n",
            "\n",
            "17. move not unless you see an advantage; use not your troops unless\n",
            "there is something to be gained; fight not unless \n",
            "CPU times: user 41.9 s, sys: 1.22 s, total: 43.1 s\n",
            "Wall time: 46.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, tokenizer, \"It's time to release the Kraken when\", num_chars=300, temperature=0.5))"
      ],
      "metadata": {
        "id": "dbQUvkQBRom9",
        "outputId": "88610fcc-d394-4637-ebb0-b19c50f5cea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 351ms/step\n",
            "1/1 [==============================] - 0s 254ms/step\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 219ms/step\n",
            "It's time to release the Kraken when the well-being of your men, and do not attack.\n",
            "\n",
            "8. (3) when the force of the flames has reached its height, follow it\n",
            "up with an attack, if that is practicable; if not, stay where\n",
            "you are.\n",
            "\n",
            "20. anger may in time change to gladness; vexation may be succeeded by\n",
            "content.\n",
            "\n",
            "21. but a kingdom that has o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, tokenizer, \"Crush your enemies, see them driven before you, and\", num_chars=300,\n",
        "                    temperature=1))"
      ],
      "metadata": {
        "id": "L9H2fw3-Sdc2",
        "outputId": "f1877601-6a9d-4445-8e64-326388cc0c09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 184ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 351ms/step\n",
            "1/1 [==============================] - 0s 211ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 223ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 232ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 128ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "Crush your enemies, see them driven before you, and we may are has\n",
            "destroyed converted spies and available for our service.\n",
            "\n",
            "22. it is through the information brought by the converted spy that we\n",
            "are able to acquire and employ local and inward spies.\n",
            "\n",
            "23. it is owing to his information, again, that we can cause the doomed\n",
            "spy. hence it is essential \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, tokenizer, \"What is best in life?\", num_chars=300, temperature=2))"
      ],
      "metadata": {
        "id": "uoM7pxbySuGG",
        "outputId": "7c88e021-a60a-4907-c52c-c620b66ade82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 291ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 465ms/step\n",
            "1/1 [==============================] - 0s 198ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 162ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 407ms/step\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 193ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 334ms/step\n",
            "1/1 [==============================] - 0s 284ms/step\n",
            "1/1 [==============================] - 0s 244ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "What is best in life?\n",
            "\n",
            "2ar confunder, the path defined battles confer.\n",
            "\n",
            "1. skowing them outlay inflisate of a sovoutint; dyet a city, or event he will be commotiom, and doome\n",
            "\n",
            "22.;t, it is auther is _doi too lü yüa who had served under the yin hapi-man. is it bskignoned\n",
            "_marruitly and remaining fie of the fourth.\n",
            "\n",
            "3. le\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, tokenizer, \"The best pub in Manchester is\", num_chars=300, temperature=0.1))"
      ],
      "metadata": {
        "id": "6VeJqgGiTWWl",
        "outputId": "8e64fe9e-3a5b-4aa5-a692-3fd4444b709c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 140ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 113ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "The best pub in Manchester is knowledge of\n",
            "the enemy; and this knowledge can only be derived, in the first\n",
            "instance, from the converted spy. hence it is essential that the\n",
            "converted spy be treated with the utmost liberality.\n",
            "\n",
            "26. of old, the rise of the yin dynasty was due to i chih who had\n",
            "served under the yin.\n",
            "\n",
            "27. hence it i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few observations of the preceding outputs:\n",
        "\n",
        "1. Despite being a character-level model, the model managed to \"learn\" spelling, cadence, punctuation, spacing, grammar, and even numbered bullet points just from trying to predict the next character.\n",
        "\n",
        "2. It's pretty cool how the model manages to take our initial seed text and complete a sentence with it before moving on.\n",
        "\n",
        "3. We can see the output getting increasingly nonsensical as the temperature rises. What temperature to use ultimately depends on the nature of your corpus and your goals with the language model.\n",
        "\n",
        "Also, in contrast to our language model, GPT-3 has 175 billion parameters and was trained on 45 terabytes of data, but the high-level principle of learning through prediction remains the same."
      ],
      "metadata": {
        "id": "PoqdEVv8Szjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Exploration\n",
        "1. Check out Sunspring, a sci-fi short written by an LSTM. The director and actors played the script straight and the result is hilarious.\n",
        "https://www.youtube.com/watch?v=LY7x2Ihqjmc\n",
        "https://en.wikipedia.org/wiki/Sunspring\n",
        "\n",
        "2. Everything we learned here can be applied at the word-level. Try creating a word-level language model with a different corpus (maybe download something from https://www.gutenberg.org/) and try using word embeddings.\n",
        "\n",
        "3. We didn't evaluate our language model using perplexity. Find out online how to do it."
      ],
      "metadata": {
        "id": "twlNt_pXS3DI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d4O-yugKS0D_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}