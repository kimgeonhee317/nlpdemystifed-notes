{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOYl+KVPuEIc1nm9XJxIs1E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimgeonhee317/nlpdemystifed-notes/blob/main/notebook/13_Recurrent_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13_Recurrent Neural Networks"
      ],
      "metadata": {
        "id": "z9u4MJ2Pjliq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Library"
      ],
      "metadata": {
        "id": "idcV9Ofejsq0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8_LFKcmAZlM9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from nltk.corpus import treebank, brown, conll2000\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part-of-Speech Tagger with Bidirectional LSTM"
      ],
      "metadata": {
        "id": "moqKQkM4kTWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PoS tagging with LSTM is multiclass classification task for sequence.\n",
        "\n",
        "# nltk offers free sets for labelled corpora.\n",
        "# look at https://www.nltk.org/nltk_data\n",
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "nltk.download('conll2000')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVb3M8AIkUFD",
        "outputId": "e333328e-14aa-411e-aa83-85785e69991e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download tagset\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf1e3Hykk10t",
        "outputId": "575978bf-50e3-40c8-addd-803d03d1ce67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download all PoS-tagged sentences and place them in one List.\n",
        "tagged_sentences = treebank.tagged_sents(tagset='universal')+\\\n",
        "                   brown.tagged_sents(tagset='universal')+\\\n",
        "                   conll2000.tagged_sents(tagset='universal')\n",
        "print(tagged_sentences[0])\n",
        "print(f\"Dataset size: {len(tagged_sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_SILV6OlagZ",
        "outputId": "552b3a18-a383-470d-af69-47d3995a055c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n",
            "Dataset size: 72202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, sentence_tags = [], []\n",
        "\n",
        "for s in tagged_sentences:\n",
        "  sentence, tags = zip(*s) # multiple numbers of tuple according to sentences\n",
        "  sentences.append(list(sentence))\n",
        "  sentence_tags.append(list(tags))"
      ],
      "metadata": {
        "id": "fsG84AxsmBrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[0])\n",
        "print(sentence_tags[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuDNua06oEzi",
        "outputId": "b90ea104-9888-4a15-ac3e-834afdc8ca73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
            "['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentences), len(sentence_tags)) # number of sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIgrWPASoJ8p",
        "outputId": "1a29ba2d-f0fd-4b49-c13d-35e6a45ee05c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72202 72202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset according to the proportion below\n",
        "train_ratio = 0.75\n",
        "validation_ratio = 0.15\n",
        "test_ratio=0.10\n",
        "\n",
        "# train:test = 0.75:0.25\n",
        "x_train, x_test, y_train, y_test = train_test_split(sentences, sentence_tags,\n",
        "                                                     test_size= 1-train_ratio,\n",
        "                                                     random_state = 317)\n",
        "# train:val:test = 0.75:0.15:0.10\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test,\n",
        "                                                test_size = test_ratio/(test_ratio + validation_ratio),\n",
        "                                                random_state = 317)"
      ],
      "metadata": {
        "id": "m64dEOEto4RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train), len(y_train))\n",
        "print(len(x_val), len(y_val))\n",
        "print(len(x_test), len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oahgHkABp6-k",
        "outputId": "b5b2a14f-6275-43ba-cd48-a6adc1e0bd4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54151 54151\n",
            "10830 10830\n",
            "7221 7221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate wordvectors for our sentenses\n",
        "# default tokenizer, out-ov-vocabulary token as <OOV>\n",
        "sentence_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<OOV')\n",
        "sentence_tokenizer.fit_on_texts(x_train)\n",
        "print(f\"Vocabulary size: {len(sentence_tokenizer.word_index)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7XFgJzTqB2u",
        "outputId": "bcecb5d1-2072-4ee8-b796-62f9a5d1481b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 52183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need another tokenizer for the tags are also sequences.\n",
        "tag_tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(y_train)"
      ],
      "metadata": {
        "id": "BtzyVYOurvYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of PoS tags: {len(tag_tokenizer.word_index)}\\n\")\n",
        "tag_tokenizer.get_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arfXHis_sioK",
        "outputId": "487e2607-c91c-4916-c6ee-1022027f61cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of PoS tags: 12\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_words': None,\n",
              " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
              " 'lower': True,\n",
              " 'split': ' ',\n",
              " 'char_level': False,\n",
              " 'oov_token': None,\n",
              " 'document_count': 54151,\n",
              " 'word_counts': '{\"adv\": 51392, \"verb\": 175631, \"adp\": 137365, \"det\": 127600, \".\": 143593, \"adj\": 81110, \"noun\": 288313, \"conj\": 35420, \"num\": 21374, \"prt\": 31340, \"pron\": 44737, \"x\": 6109}',\n",
              " 'word_docs': '{\"conj\": 24581, \"det\": 44815, \"verb\": 50880, \"num\": 11905, \"adj\": 36440, \"noun\": 51202, \"adp\": 43937, \"adv\": 29599, \".\": 53331, \"prt\": 21888, \"pron\": 26974, \"x\": 2668}',\n",
              " 'index_docs': '{\"9\": 24581, \"5\": 44815, \"2\": 50880, \"11\": 11905, \"6\": 36440, \"1\": 51202, \"4\": 43937, \"7\": 29599, \"3\": 53331, \"10\": 21888, \"8\": 26974, \"12\": 2668}',\n",
              " 'index_word': '{\"1\": \"noun\", \"2\": \"verb\", \"3\": \".\", \"4\": \"adp\", \"5\": \"det\", \"6\": \"adj\", \"7\": \"adv\", \"8\": \"pron\", \"9\": \"conj\", \"10\": \"prt\", \"11\": \"num\", \"12\": \"x\"}',\n",
              " 'word_index': '{\"noun\": 1, \"verb\": 2, \".\": 3, \"adp\": 4, \"det\": 5, \"adj\": 6, \"adv\": 7, \"pron\": 8, \"conj\": 9, \"prt\": 10, \"num\": 11, \"x\": 12}'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwklrTqusjkL",
        "outputId": "0a8e59c7-1cc1-4f19-feeb-1aed5e92ed63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'noun': 1,\n",
              " 'verb': 2,\n",
              " '.': 3,\n",
              " 'adp': 4,\n",
              " 'det': 5,\n",
              " 'adj': 6,\n",
              " 'adv': 7,\n",
              " 'pron': 8,\n",
              " 'conj': 9,\n",
              " 'prt': 10,\n",
              " 'num': 11,\n",
              " 'x': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert text to number sequences by fitted tokenizer\n",
        "x_train_seqs = sentence_tokenizer.texts_to_sequences(x_train)"
      ],
      "metadata": {
        "id": "4vYvnVhTtmOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_seqs[0])\n",
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41PgV2eRt0BP",
        "outputId": "a42608eb-31ad-482c-d7a3-c17214164080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[133, 1921, 19, 8, 13, 1606, 461, 15, 4344, 318, 12, 8, 1922, 28157, 1858, 25, 12287, 28158, 926, 926, 20609, 7, 6603, 6, 20610, 1568, 9, 3198, 39, 13, 145, 6, 1742, 23, 834, 295, 3038, 311, 28159, 32, 28160, 1284, 3, 11, 207, 20609, 1742, 429, 2072, 12288, 15, 4]\n",
            "['Still', 'existing', 'on', 'a', '``', 'Northern', 'Union', \"''\", 'telegraph', 'form', 'is', 'a', 'typical', 'peremptory', 'message', 'from', 'Peru', 'grocer', 'J.', 'J.', 'Hapgood', 'to', 'Burton', 'and', \"Graves'\", 'store', 'in', 'Manchester', '--', '``', 'Get', 'and', 'send', 'by', 'stage', 'four', 'pounds', 'best', 'Porterhouse', 'or', 'serloin', 'stake', ',', 'for', 'Mrs.', 'Hapgood', 'send', 'six', 'sweet', 'oranges', \"''\", '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_seqs = tag_tokenizer.texts_to_sequences(y_train)"
      ],
      "metadata": {
        "id": "OIwxOqfdt4D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tag_tokenizer.sequences_to_texts([y_train_seqs[0]]))\n",
        "print(y_train_seqs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRyi-i-suIjq",
        "outputId": "a0ad622f-edc7-4e80-ea8f-8db4fba6d5bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['adv verb adp det . adj noun . noun noun verb det adj adj noun adp noun noun noun noun noun adp noun conj noun noun adp noun . . verb conj verb adp noun num noun adj noun conj noun noun . adp noun noun verb num adj noun . .']\n",
            "[7, 2, 4, 5, 3, 6, 1, 3, 1, 1, 2, 5, 6, 6, 1, 4, 1, 1, 1, 1, 1, 4, 1, 9, 1, 1, 4, 1, 3, 3, 2, 9, 2, 4, 1, 11, 1, 6, 1, 9, 1, 1, 3, 4, 1, 1, 2, 11, 6, 1, 3, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the same things to valid dataset\n",
        "x_val_seqs = sentence_tokenizer.texts_to_sequences(x_val)\n",
        "y_val_seqs = tag_tokenizer.texts_to_sequences(y_val)"
      ],
      "metadata": {
        "id": "7Vz817BkuSbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Even if RNN can handle variable lengthes of sequences, it is musch better for performance to unify the lengthes of each sequences\n",
        "print(len(max(x_train_seqs, key=len))) # return the length of the longest sequence\n",
        "MAX_LENGTH = len(max(x_train_seqs, key=len))\n",
        "print(f\"Length of longest input sequence: {MAX_LENGTH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5DE6_sOu4nV",
        "outputId": "aab55973-4d1c-4d50-9110-cac7804bc505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "271\n",
            "Length of longest input sequence: 271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can pad every sentences with method \"pad_sequences\" from keras\n",
        "x_train_padded = keras.preprocessing.sequence.pad_sequences(x_train_seqs, padding='post',\n",
        "                                                            maxlen=MAX_LENGTH)\n"
      ],
      "metadata": {
        "id": "WvjMFVPcviLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_padded[0])\n",
        "print(len(x_train_padded[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIC3PRHuy1JK",
        "outputId": "4394167d-dea2-4e10-fd54-db753a14a97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  133  1921    19     8    13  1606   461    15  4344   318    12     8\n",
            "  1922 28157  1858    25 12287 28158   926   926 20609     7  6603     6\n",
            " 20610  1568     9  3198    39    13   145     6  1742    23   834   295\n",
            "  3038   311 28159    32 28160  1284     3    11   207 20609  1742   429\n",
            "  2072 12288    15     4     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0]\n",
            "271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do the same things to training label\n",
        "y_train_padded = keras.preprocessing.sequence.pad_sequences(y_train_seqs, padding='post',\n",
        "                                                           maxlen=MAX_LENGTH)"
      ],
      "metadata": {
        "id": "PvY-cmPZy2zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val_padded = keras.preprocessing.sequence.pad_sequences(x_val_seqs, padding='post', maxlen=MAX_LENGTH)\n",
        "y_val_padded = keras.preprocessing.sequence.pad_sequences(y_val_seqs, padding='post', maxlen=MAX_LENGTH)"
      ],
      "metadata": {
        "id": "DTRgOeoXzHnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As PoS tagging is a multiclass classification task done at each timestep,\n",
        "# we need to convert every tag for every sentence into one-hot encoding.\n",
        "y_train_categoricals = keras.utils.to_categorical(y_train_padded)\n",
        "print(y_train_categoricals[0]) # sequence is now composed of one-hot encodings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlg3RsmtzUoN",
        "outputId": "cc08f9b4-84b0-4a8f-cab6-f7864f5f2c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encoding for a single tag in a sequence\n",
        "print(y_train_categoricals[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENGlUOJjzt3J",
        "outputId": "34bcecbb-5c71-49dc-f485-b57bd3d0be23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can determind PoS tag from oh-encoding by \"look-up\" in index_word dictionary from tag_tokenizer\n",
        "idx = np.argmax(y_train_categoricals[0][0]) # argmax return the index of elememt having maximum value in OHencoding array\n",
        "print(f\"Index: {idx}\")\n",
        "print(f\"Tag: {tag_tokenizer.index_word[idx]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsctW0Fm0gLH",
        "outputId": "2ba2f708-f72f-43b4-9d8c-f74c3ea9f135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 7\n",
            "Tag: adv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encoding to val_labels\n",
        "y_val_categoricals = keras.utils.to_categorical(y_val_padded)"
      ],
      "metadata": {
        "id": "Gu4HHrym1E4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[notes]\n",
        "At this point, we're ready to build our model. We'll train word embeddings concurrently with our model (though you can use pretrained word vectors as well)."
      ],
      "metadata": {
        "id": "CQ_OyvCSzyKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[notes]\n",
        "1. Ignore padding values :\n",
        "The embedding layers has *mask_zero* parameter. we added padding in order to make our batches the same size, but we don't want to makd PoS predictions on padding. Setting *mask_zero* to *True* makes the layers following the embedding layer ignore padding values.\n",
        "\n",
        "2. Return sequences not only one output :\n",
        "we're using *bidriectional LSTM*. The Bidrectional layer is a wrapper to which we pass an LSTM layer. The first parameter to the LSTM layer is the number of units in the cell. The second parameter, return_sequences, control whether the RNN returns an output for each timestep or only the last output. Since we're doing PoS-tagging, we want an aoutput for each timestep and so *return_sequences* is set to *True*."
      ],
      "metadata": {
        "id": "xepXLukU2YB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the embedding layer. \"+1\" to account for the padding token.\n",
        "num_tokens = len(sentence_tokenizer.word_index) + 1 # +1 for padding token\n",
        "embedding_dim = 128\n",
        "\n",
        "# For the output layer, The number of classes corresponds to the number of possible tags\n",
        "num_classes = len(tag_tokenizer.word_index) + 1 # also +1 for padding token"
      ],
      "metadata": {
        "id": "x63KC0tG4Oaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we set random_set_seed and kerner_initializer parameter to get same result.\n",
        "tf.random.set_seed(317)\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# input layer(embedding layer : each tokens -> embedding_dim )\n",
        "model.add(layers.Embedding(input_dim = num_tokens,\n",
        "                           output_dim = embedding_dim,\n",
        "                           input_length = MAX_LENGTH,\n",
        "                           mask_zero=True))\n",
        "\n",
        "# hidden layer (bidrectional)\n",
        "model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True,\n",
        "                                           kernel_initializer=tf.keras.initializers.random_normal(seed=317))))\n",
        "\n",
        "# output layer for each timestep with softmax activation fucntion\n",
        "model.add(layers.Dense(num_classes, activation='softmax',\n",
        "                       kernel_initializer=tf.keras.initializers.random_normal(seed=317)))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "CX8V0jM057Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kzrHF588OJi",
        "outputId": "bebe1f46-331c-4d47-cbb6-2ecd0f05ff67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 271, 128)          6679552   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 271, 256)         263168    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 271, 13)           3341      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,946,061\n",
            "Trainable params: 6,946,061\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[notes] \\\n",
        "1. The embedding layer output has three dimensions\n",
        "- Batch size : None => we haven't specified it yet\n",
        "- Sequence length : 217\n",
        "- Embedding dimension : 128\n",
        "\n",
        "2. Bidirectional LSTM outputs a vector twice the size of what we specified because its bidirectional. Remember two LSTM output will be concatenated before going to output layer.\n",
        "\n",
        "3. Output layer also has three dimensions\n",
        "- Batch size\n",
        "- Sequence length\n",
        "- Output dimension : 13 as number of tag classes\n"
      ],
      "metadata": {
        "id": "U0WjEnpT7w1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we put early-stopping for trainig to be stopped when validation loss stops improving.\n",
        "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history = model.fit(x_train_padded, y_train_categoricals, epochs=20,\n",
        "                    batch_size=256, validation_data=(x_val_padded, y_val_categoricals),\n",
        "                    callbacks=[es_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMSCaB708ZO1",
        "outputId": "8477a915-a062-4039-c8e4-eca7efeed24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "212/212 [==============================] - 65s 239ms/step - loss: 0.9598 - accuracy: 0.7057 - val_loss: 0.1617 - val_accuracy: 0.9512\n",
            "Epoch 2/20\n",
            "212/212 [==============================] - 25s 117ms/step - loss: 0.1138 - accuracy: 0.9642 - val_loss: 0.1082 - val_accuracy: 0.9642\n",
            "Epoch 3/20\n",
            "212/212 [==============================] - 17s 81ms/step - loss: 0.0753 - accuracy: 0.9753 - val_loss: 0.0968 - val_accuracy: 0.9678\n",
            "Epoch 4/20\n",
            "212/212 [==============================] - 20s 94ms/step - loss: 0.0595 - accuracy: 0.9803 - val_loss: 0.0944 - val_accuracy: 0.9692\n",
            "Epoch 5/20\n",
            "212/212 [==============================] - 14s 66ms/step - loss: 0.0480 - accuracy: 0.9842 - val_loss: 0.0924 - val_accuracy: 0.9703\n",
            "Epoch 6/20\n",
            "212/212 [==============================] - 12s 57ms/step - loss: 0.0392 - accuracy: 0.9875 - val_loss: 0.0954 - val_accuracy: 0.9698\n",
            "Epoch 7/20\n",
            "212/212 [==============================] - 12s 55ms/step - loss: 0.0322 - accuracy: 0.9898 - val_loss: 0.0995 - val_accuracy: 0.9701\n",
            "Epoch 8/20\n",
            "212/212 [==============================] - 12s 55ms/step - loss: 0.0265 - accuracy: 0.9918 - val_loss: 0.1026 - val_accuracy: 0.9700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After our model is trained, go to test data set\n",
        "# preprocessing(tokenize, pad) it and oh encoding\n",
        "\n",
        "x_test_seqs = sentence_tokenizer.texts_to_sequences(x_test)\n",
        "x_test_padded = keras.preprocessing.sequence.pad_sequences(x_test_seqs, padding='post', maxlen=MAX_LENGTH)\n",
        "\n",
        "y_test_seqs = tag_tokenizer.texts_to_sequences(y_test)\n",
        "y_test_padded = keras.preprocessing.sequence.pad_sequences(y_test_seqs, padding='post', maxlen=MAX_LENGTH)\n",
        "y_test_categoricals = keras.utils.to_categorical(y_test_padded)"
      ],
      "metadata": {
        "id": "QqxYjmQn9ve9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test_padded, y_test_categoricals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n4n9Jly-ruD",
        "outputId": "2607734f-36d7-4a4a-cc63-98522d620bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226/226 [==============================] - 2s 10ms/step - loss: 0.1052 - accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1051989421248436, 0.9687950611114502]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can use our models to tag sentences.\n",
        "\n",
        "samples = [\n",
        "    \"Brown refused to testify.\",\n",
        "    \"Brown sofas are on sale\",\n",
        "]"
      ],
      "metadata": {
        "id": "_Ac9odm9-wJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# develop simple function for doing this task.\n",
        "\n",
        "def tag_sentences(sentences):\n",
        "  sentences_seqs = sentence_tokenizer.texts_to_sequences(sentences)\n",
        "  sentences_padded = keras.preprocessing.sequence.pad_sequences(sentences_seqs,\n",
        "                                                               maxlen=MAX_LENGTH,\n",
        "                                                               padding='post')\n",
        "  # tag_preds is each list of probabilty distribution (softmax)\n",
        "  tag_preds = model.predict(sentences_padded)\n",
        "\n",
        "  sentence_tags = []\n",
        "\n",
        "  # each iteration is one sequence\n",
        "  for i, preds in enumerate(tag_preds):\n",
        "\n",
        "    print(preds)\n",
        "    # seq of most probable ones in sequence\n",
        "    tags_seq = [np.argmax(p) for p in preds[:len(sentences_seqs[i])]]\n",
        "    words = [sentence_tokenizer.index_word[w] for w in sentences_seqs[i]]\n",
        "    tags = [tag_tokenizer.index_word[t] for t in tags_seq]\n",
        "    sentence_tags.append(list(zip(words, tags)))\n",
        "\n",
        "  return sentence_tags"
      ],
      "metadata": {
        "id": "fjDS3PHZ_DHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_sample_sentences = tag_sentences(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot1Wu41vAKZ2",
        "outputId": "766dfe38-e775-420f-c057-e8fa0e7b94a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "[[3.2334467e-06 9.9331737e-01 2.9946794e-04 ... 3.9344709e-04\n",
            "  1.2747691e-05 2.2187696e-05]\n",
            " [1.2938746e-06 3.1313516e-04 9.9806172e-01 ... 1.4160351e-04\n",
            "  4.5686566e-09 1.3718624e-06]\n",
            " [1.2729881e-06 2.0816846e-05 6.1781978e-04 ... 4.8411652e-01\n",
            "  3.2293276e-05 3.9092065e-06]\n",
            " ...\n",
            " [7.5027689e-02 8.0436386e-02 7.9266421e-02 ... 7.5670168e-02\n",
            "  7.5860150e-02 7.5894080e-02]\n",
            " [7.5027689e-02 8.0436386e-02 7.9266421e-02 ... 7.5670168e-02\n",
            "  7.5860150e-02 7.5894080e-02]\n",
            " [7.5027689e-02 8.0436386e-02 7.9266421e-02 ... 7.5670168e-02\n",
            "  7.5860150e-02 7.5894080e-02]]\n",
            "[[4.2764573e-06 1.5268865e-01 1.3265687e-04 ... 3.5671474e-05\n",
            "  2.9227600e-04 1.2731762e-05]\n",
            " [2.3185294e-08 9.9997950e-01 6.1821049e-07 ... 1.7081586e-06\n",
            "  1.0583433e-06 1.6246985e-06]\n",
            " [4.9634735e-10 3.8132605e-06 9.9999332e-01 ... 1.6310411e-08\n",
            "  2.7530690e-13 1.3395328e-08]\n",
            " ...\n",
            " [7.5027689e-02 8.0436386e-02 7.9266421e-02 ... 7.5670168e-02\n",
            "  7.5860150e-02 7.5894080e-02]\n",
            " [7.5027689e-02 8.0436386e-02 7.9266421e-02 ... 7.5670168e-02\n",
            "  7.5860150e-02 7.5894080e-02]\n",
            " [7.5027689e-02 8.0436386e-02 7.9266421e-02 ... 7.5670168e-02\n",
            "  7.5860150e-02 7.5894080e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tagged_sample_sentences[0])\n",
        "print(tagged_sample_sentences[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRumHAfjAOcF",
        "outputId": "b1088276-0b8c-4334-e981-9d76f27ac458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('brown', 'noun'), ('refused', 'verb'), ('to', 'adp'), ('testify', 'verb')]\n",
            "[('brown', 'adj'), ('sofas', 'noun'), ('are', 'verb'), ('on', 'adp'), ('sale', 'noun')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's just one way of buidling a PoS tagger, these days' PoS tagger is much more sophisticated models in which transfomer is used."
      ],
      "metadata": {
        "id": "sYL-XcXyBwN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Modelling With Stacked LSTMs"
      ],
      "metadata": {
        "id": "7aNbJ1L77w3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download sample corpus\n",
        "art_of_war = requests.get('https://raw.githubusercontent.com/nitinpunjabi/nlp-demystified/main/datasets/art_of_war.txt')\\\n",
        "                     .text\n",
        "\n",
        "art_of_war[:300]"
      ],
      "metadata": {
        "id": "yn1fuCFAB6fo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8f0b586d-d4c8-4d35-92e3-7e29cc378d1e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1. Sun Tzŭ said: The art of war is of vital importance to the State.\\n\\n2. It is a matter of life and death, a road either to safety or to\\nruin. Hence it is a subject of inquiry which can on no account be\\nneglected.\\n\\n3. The art of war, then, is governed by five constant factors, to be\\ntaken into accou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[notes]\\\n",
        "We will build character-based language model(opposed to word-based)\n",
        "\n",
        "Character-level models have the advantage of:\n",
        "1. Smaller prediction space (smaller character than words.. in english obviously)\n",
        "2. More resilient to the out-of-vocabulary(OOV) problem. and better able to learn lower mechanics of language (including punctuation)\n",
        "\n",
        "On the other hand, character-level models need to learn a sequence of charcters to \"make sense\" of a word (e.g. the sequence of \"c\", \"a\", \"t\" to identify \"cat\" as a pattern) which can be inefficient and result in lower performance.\n",
        "\n",
        "RNNs can process any kind of sequence so what's shown here can easily be applied at the word level. When we cover transformers, there is an alterniative approaches called subword tokenization which is the moddle-ground between these two approaches.\n"
      ],
      "metadata": {
        "id": "mksVVK2b8db_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level = True)"
      ],
      "metadata": {
        "id": "IJvSKHAj8STw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([art_of_war])"
      ],
      "metadata": {
        "id": "UfJi-2Gd91V8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_config()"
      ],
      "metadata": {
        "id": "mBOBdC3r97yF",
        "outputId": "3fcb8a76-568a-428d-8125-b8b9042c4736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_words': None,\n",
              " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
              " 'lower': True,\n",
              " 'split': ' ',\n",
              " 'char_level': True,\n",
              " 'oov_token': None,\n",
              " 'document_count': 1,\n",
              " 'word_counts': '{\"1\": 179, \".\": 896, \" \": 9794, \"s\": 3081, \"u\": 1467, \"n\": 3565, \"t\": 4398, \"z\": 20, \"\\\\u016d\": 13, \"a\": 3475, \"i\": 3573, \"d\": 1681, \":\": 48, \"h\": 2558, \"e\": 5837, \"r\": 2776, \"o\": 3548, \"f\": 1238, \"w\": 981, \"v\": 478, \"l\": 1722, \"m\": 1201, \"p\": 769, \"c\": 1390, \"\\\\n\": 1443, \"2\": 127, \",\": 634, \"y\": 1055, \"b\": 708, \"j\": 23, \"q\": 55, \"g\": 1007, \"3\": 87, \"k\": 345, \"\\\\u2019\": 57, \"4\": 66, \"(\": 59, \")\": 59, \";\": 168, \"5\": 58, \"6\": 51, \"_\": 62, \"7\": 39, \"8\": 36, \"9\": 34, \"0\": 38, \"x\": 49, \"\\\\u2014\": 16, \"?\": 8, \"!\": 8, \"-\": 57, \"\\\\u201c\": 3, \"\\\\u201d\": 3, \"\\\\u0153\": 7, \"\\\\u00fc\": 3, \"\\\\u2018\": 1}',\n",
              " 'word_docs': '{\"e\": 1, \"\\\\u016d\": 1, \"6\": 1, \"j\": 1, \"w\": 1, \"c\": 1, \"y\": 1, \"7\": 1, \"5\": 1, \"\\\\u2018\": 1, \"p\": 1, \"g\": 1, \"2\": 1, \"k\": 1, \" \": 1, \"b\": 1, \"0\": 1, \"m\": 1, \"\\\\u201c\": 1, \"s\": 1, \"4\": 1, \"9\": 1, \"?\": 1, \"o\": 1, \"r\": 1, \";\": 1, \"v\": 1, \"q\": 1, \"!\": 1, \"\\\\u00fc\": 1, \"h\": 1, \"l\": 1, \"(\": 1, \"a\": 1, \"3\": 1, \"u\": 1, \"\\\\u2019\": 1, \"8\": 1, \")\": 1, \"x\": 1, \":\": 1, \"\\\\n\": 1, \"_\": 1, \"z\": 1, \"t\": 1, \".\": 1, \"i\": 1, \"f\": 1, \"d\": 1, \"1\": 1, \"\\\\u0153\": 1, \"n\": 1, \",\": 1, \"-\": 1, \"\\\\u2014\": 1, \"\\\\u201d\": 1}',\n",
              " 'index_docs': '{\"2\": 1, \"49\": 1, \"39\": 1, \"46\": 1, \"20\": 1, \"15\": 1, \"18\": 1, \"42\": 1, \"35\": 1, \"56\": 1, \"22\": 1, \"19\": 1, \"29\": 1, \"26\": 1, \"1\": 1, \"23\": 1, \"43\": 1, \"17\": 1, \"53\": 1, \"8\": 1, \"31\": 1, \"45\": 1, \"50\": 1, \"6\": 1, \"9\": 1, \"28\": 1, \"25\": 1, \"38\": 1, \"51\": 1, \"55\": 1, \"10\": 1, \"11\": 1, \"33\": 1, \"7\": 1, \"30\": 1, \"13\": 1, \"36\": 1, \"44\": 1, \"34\": 1, \"40\": 1, \"41\": 1, \"14\": 1, \"32\": 1, \"47\": 1, \"3\": 1, \"21\": 1, \"4\": 1, \"16\": 1, \"12\": 1, \"27\": 1, \"52\": 1, \"5\": 1, \"24\": 1, \"37\": 1, \"48\": 1, \"54\": 1}',\n",
              " 'index_word': '{\"1\": \" \", \"2\": \"e\", \"3\": \"t\", \"4\": \"i\", \"5\": \"n\", \"6\": \"o\", \"7\": \"a\", \"8\": \"s\", \"9\": \"r\", \"10\": \"h\", \"11\": \"l\", \"12\": \"d\", \"13\": \"u\", \"14\": \"\\\\n\", \"15\": \"c\", \"16\": \"f\", \"17\": \"m\", \"18\": \"y\", \"19\": \"g\", \"20\": \"w\", \"21\": \".\", \"22\": \"p\", \"23\": \"b\", \"24\": \",\", \"25\": \"v\", \"26\": \"k\", \"27\": \"1\", \"28\": \";\", \"29\": \"2\", \"30\": \"3\", \"31\": \"4\", \"32\": \"_\", \"33\": \"(\", \"34\": \")\", \"35\": \"5\", \"36\": \"\\\\u2019\", \"37\": \"-\", \"38\": \"q\", \"39\": \"6\", \"40\": \"x\", \"41\": \":\", \"42\": \"7\", \"43\": \"0\", \"44\": \"8\", \"45\": \"9\", \"46\": \"j\", \"47\": \"z\", \"48\": \"\\\\u2014\", \"49\": \"\\\\u016d\", \"50\": \"?\", \"51\": \"!\", \"52\": \"\\\\u0153\", \"53\": \"\\\\u201c\", \"54\": \"\\\\u201d\", \"55\": \"\\\\u00fc\", \"56\": \"\\\\u2018\"}',\n",
              " 'word_index': '{\" \": 1, \"e\": 2, \"t\": 3, \"i\": 4, \"n\": 5, \"o\": 6, \"a\": 7, \"s\": 8, \"r\": 9, \"h\": 10, \"l\": 11, \"d\": 12, \"u\": 13, \"\\\\n\": 14, \"c\": 15, \"f\": 16, \"m\": 17, \"y\": 18, \"g\": 19, \"w\": 20, \".\": 21, \"p\": 22, \"b\": 23, \",\": 24, \"v\": 25, \"k\": 26, \"1\": 27, \";\": 28, \"2\": 29, \"3\": 30, \"4\": 31, \"_\": 32, \"(\": 33, \")\": 34, \"5\": 35, \"\\\\u2019\": 36, \"-\": 37, \"q\": 38, \"6\": 39, \"x\": 40, \":\": 41, \"7\": 42, \"0\": 43, \"8\": 44, \"9\": 45, \"j\": 46, \"z\": 47, \"\\\\u2014\": 48, \"\\\\u016d\": 49, \"?\": 50, \"!\": 51, \"\\\\u0153\": 52, \"\\\\u201c\": 53, \"\\\\u201d\": 54, \"\\\\u00fc\": 55, \"\\\\u2018\": 56}'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tokenizer \\\"Vocabulary\\\" size : {len(tokenizer.word_index)}\")"
      ],
      "metadata": {
        "id": "ZYrlMioO9_Cn",
        "outputId": "e8f05612-1fd6-443c-eb88-ec76e6770b1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer \"Vocabulary\" size : 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq = tokenizer.texts_to_sequences([art_of_war])[0] # first sequence"
      ],
      "metadata": {
        "id": "9SsU36W4-WdL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Text length: {len(seq)}\")"
      ],
      "metadata": {
        "id": "063vMdCZ-mIq",
        "outputId": "6deceb7d-6425-4e82-d079-a42caf5eaea8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text length: 61054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check.\n",
        "tokenizer.sequences_to_texts([seq[:10]])"
      ],
      "metadata": {
        "id": "W36mbIrV-ptA",
        "outputId": "a1da0289-b0de-4ba1-879b-81104ed32672",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1 .   s u n   t z ŭ']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll gonna use \"Tensorflow Data API\" which makes it easy to build preprocessing pipelines by chaining operations together\n",
        "# To use this, we need to converted our vectorized courpus into Dataset object (from_tensor_slices)\n",
        "slices = tf.data.Dataset.from_tensor_slices(seq)\n",
        "type(slices)"
      ],
      "metadata": {
        "id": "A_x1HfLe-6Hk",
        "outputId": "f57711d2-9083-4fd4-d5e0-ec06ae28a957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.from_tensor_slices_op._TensorSliceDataset"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(slices.take(10))"
      ],
      "metadata": {
        "id": "idGOsXMn_f5a",
        "outputId": "f3a8b772-ac98-458d-ff81-64675115cfb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=int32, numpy=27>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=21>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=8>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=13>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=5>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=3>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=47>,\n",
              " <tf.Tensor: shape=(), dtype=int32, numpy=49>]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq[:10]"
      ],
      "metadata": {
        "id": "ah1LF2Du_kiq",
        "outputId": "0ee299d9-a54f-4533-85fd-55c85a01c27a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[27, 21, 1, 8, 13, 5, 1, 3, 47, 49]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll use window method. window size will be 100. Dataset of 1000 willbe sequence of 10 datasets, each containing 100 elements\n",
        "# Here, we create window \"input_timesteps + 1\" which is for target/label for each training example.\n",
        "# Shift to 1, window shift tactics.\n",
        "# Drop_remainder to True which ensure All windows contain exactly N elements. i.e. once input contians fewer than N, it dropped.\n",
        "input_timesteps = 100\n",
        "window_size = input_timesteps + 1\n",
        "windows = slices.window(window_size, shift=1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "meDEWgsW_mjg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in windows.take(3):\n",
        "  arr = list(w.as_numpy_iterator())\n",
        "  print(len(arr), arr)"
      ],
      "metadata": {
        "id": "JHlKluGpA0yi",
        "outputId": "70d2ebe1-06af-4a31-8597-a85ce419660b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 [27, 21, 1, 8, 13, 5, 1, 3, 47, 49, 1, 8, 7, 4, 12, 41, 1, 3, 10, 2, 1, 7, 9, 3, 1, 6, 16, 1, 20, 7, 9, 1, 4, 8, 1, 6, 16, 1, 25, 4, 3, 7, 11, 1, 4, 17, 22, 6, 9, 3, 7, 5, 15, 2, 1, 3, 6, 1, 3, 10, 2, 1, 8, 3, 7, 3, 2, 21, 14, 14, 29, 21, 1, 4, 3, 1, 4, 8, 1, 7, 1, 17, 7, 3, 3, 2, 9, 1, 6, 16, 1, 11, 4, 16, 2, 1, 7, 5, 12, 1, 12]\n",
            "101 [21, 1, 8, 13, 5, 1, 3, 47, 49, 1, 8, 7, 4, 12, 41, 1, 3, 10, 2, 1, 7, 9, 3, 1, 6, 16, 1, 20, 7, 9, 1, 4, 8, 1, 6, 16, 1, 25, 4, 3, 7, 11, 1, 4, 17, 22, 6, 9, 3, 7, 5, 15, 2, 1, 3, 6, 1, 3, 10, 2, 1, 8, 3, 7, 3, 2, 21, 14, 14, 29, 21, 1, 4, 3, 1, 4, 8, 1, 7, 1, 17, 7, 3, 3, 2, 9, 1, 6, 16, 1, 11, 4, 16, 2, 1, 7, 5, 12, 1, 12, 2]\n",
            "101 [1, 8, 13, 5, 1, 3, 47, 49, 1, 8, 7, 4, 12, 41, 1, 3, 10, 2, 1, 7, 9, 3, 1, 6, 16, 1, 20, 7, 9, 1, 4, 8, 1, 6, 16, 1, 25, 4, 3, 7, 11, 1, 4, 17, 22, 6, 9, 3, 7, 5, 15, 2, 1, 3, 6, 1, 3, 10, 2, 1, 8, 3, 7, 3, 2, 21, 14, 14, 29, 21, 1, 4, 3, 1, 4, 8, 1, 7, 1, 17, 7, 3, 3, 2, 9, 1, 6, 16, 1, 11, 4, 16, 2, 1, 7, 5, 12, 1, 12, 2, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# window method returns a nested dataset of datasets\n",
        "print(windows, '\\n')\n",
        "\n",
        "for w in windows.take(2):\n",
        "  print(w)"
      ],
      "metadata": {
        "id": "JJgkRDvSBBIx",
        "outputId": "671460b7-be3c-4e7a-dfae-a54fbfacde94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_WindowDataset element_spec=DatasetSpec(TensorSpec(shape=(), dtype=tf.int32, name=None), TensorShape([]))> \n",
            "\n",
            "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
            "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our model only accept tensors. We need to extract tensors from each window using 'flat_map'\n",
        "dataset = windows.flat_map(lambda window: window.batch(window_size))"
      ],
      "metadata": {
        "id": "TxCZ1fTRBO4T"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we have single dataset of tensors, where each tensor is \"input_timesteps+1\" long and shifted by 1.\n",
        "for d in dataset.take(2):\n",
        "  print(d)"
      ],
      "metadata": {
        "id": "Y60dbqMXBwax",
        "outputId": "b6c41b5c-465b-42ff-b63e-7f72da2a6a24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[27 21  1  8 13  5  1  3 47 49  1  8  7  4 12 41  1  3 10  2  1  7  9  3\n",
            "  1  6 16  1 20  7  9  1  4  8  1  6 16  1 25  4  3  7 11  1  4 17 22  6\n",
            "  9  3  7  5 15  2  1  3  6  1  3 10  2  1  8  3  7  3  2 21 14 14 29 21\n",
            "  1  4  3  1  4  8  1  7  1 17  7  3  3  2  9  1  6 16  1 11  4 16  2  1\n",
            "  7  5 12  1 12], shape=(101,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[21  1  8 13  5  1  3 47 49  1  8  7  4 12 41  1  3 10  2  1  7  9  3  1\n",
            "  6 16  1 20  7  9  1  4  8  1  6 16  1 25  4  3  7 11  1  4 17 22  6  9\n",
            "  3  7  5 15  2  1  3  6  1  3 10  2  1  8  3  7  3  2 21 14 14 29 21  1\n",
            "  4  3  1  4  8  1  7  1 17  7  3  3  2  9  1  6 16  1 11  4 16  2  1  7\n",
            "  5 12  1 12  2], shape=(101,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create batches from our data set. shuffle and create!\n",
        "batch_size = 32\n",
        "batches = dataset.shuffle(10000).batch(batch_size)"
      ],
      "metadata": {
        "id": "Tqsm3wwXB_Yq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b in batches.take(2):\n",
        "  print(b)"
      ],
      "metadata": {
        "id": "AB_odxN3CLnu",
        "outputId": "0693670d-2446-4785-cffb-3a420daf8ce3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 2  1  6 ... 17 18  1]\n",
            " [18  4  5 ... 10  4  8]\n",
            " [ 1 12  4 ...  2  9  4]\n",
            " ...\n",
            " [ 2  5 19 ... 36  8  1]\n",
            " [ 1  3  6 ... 10  4 19]\n",
            " [13  8  7 ...  1 10  6]], shape=(32, 101), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[ 1  4  8 ...  4 15  2]\n",
            " [17  7  5 ...  2  1 10]\n",
            " [ 3  7  5 ... 15  3  6]\n",
            " ...\n",
            " [13  8  3 ... 24  1 20]\n",
            " [ 9  4  8 ...  1  6 16]\n",
            " [ 3  9  2 ...  1  4  5]], shape=(32, 101), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[notes]\\\n",
        "We talked about _Teacher Forcing_\n",
        "1. At each timestep during training, the output is compared to a label\n",
        "2. At the next timestep, rather than feeding the model the previous output, we feed it the next character of input sequence(i.e. what model should've outputted - label!)\n",
        "\n",
        "This is why each sequence is of size _input_timestep+1_. Each sequence is now going to be separated into Two sequences. The first sequence will be training input and will be length input_timepsteps. The second sequence will be the label/target and will consist of all the sequence elements shifted by 1.\n",
        "\n",
        "So if processed sequence is \"she swam in the lake\", then:\n",
        "+ the input will be \"she swam in the lak\" (drop last char)\n",
        "+ the target/label will be \"he swam in the lake\" (drop first char)\n"
      ],
      "metadata": {
        "id": "N2lBrbpxCVpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x-y paired bathces\n",
        "xy_batches = batches.map(lambda batch: (batch[:, :-1], batch[:, 1:]))"
      ],
      "metadata": {
        "id": "SbbF6rlsCODz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b in xy_batches.take(1):\n",
        "  print(b)"
      ],
      "metadata": {
        "id": "T3BgobHRELlG",
        "outputId": "2810d9d0-ebf8-4f97-f7c0-70e471b5765e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
            "array([[ 5,  8,  1, ...,  6,  5,  8],\n",
            "       [ 1,  9, 13, ..., 26,  1, 10],\n",
            "       [ 7, 11,  1, ..., 21,  1, 20],\n",
            "       ...,\n",
            "       [ 6,  5,  8, ...,  4,  6,  5],\n",
            "       [14,  5,  2, ..., 13,  5,  3],\n",
            "       [25,  7,  5, ..., 14, 33, 35]], dtype=int32)>, <tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
            "array([[ 8,  1,  4, ...,  5,  8,  1],\n",
            "       [ 9, 13, 11, ...,  1, 10,  4],\n",
            "       [11,  1,  6, ...,  1, 20, 10],\n",
            "       ...,\n",
            "       [ 5,  8,  1, ...,  6,  5,  8],\n",
            "       [ 5,  2, 19, ...,  5,  3,  1],\n",
            "       [ 7,  5,  3, ..., 33, 35, 34]], dtype=int32)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For greater clarity."
      ],
      "metadata": {
        "id": "qMnsCRHGEXp7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HUDZO-2WEn8x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}